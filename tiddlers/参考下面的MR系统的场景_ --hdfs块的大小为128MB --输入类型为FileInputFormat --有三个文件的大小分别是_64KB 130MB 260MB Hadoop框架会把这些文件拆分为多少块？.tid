created: 20220521053723671
difficulty: 2.2556019210869556
due: 20220826030913503
grade: 2
history: [{"due":"20220524023854932","interval":0,"difficulty":5,"stability":2,"retrievability":1,"grade":-1,"lapses":0,"reps":1,"review":"20220522023854932"},{"due":"20220527234203911","interval":1,"difficulty":4.148683298050514,"stability":4.03183163653078,"retrievability":0.9486832980505138,"grade":2,"lapses":0,"reps":2,"review":"20220523234203911"},{"due":"20220622005743146","interval":8,"difficulty":3.1600319838664648,"stability":21.015749968210223,"retrievability":0.8113486858159505,"grade":2,"lapses":0,"reps":3,"review":"20220601005743146"}]
interval: 22
lapses: 0
modified: 20220623030913503
reps: 4
retrievability: 0.8955699372204909
review: 20220623030913502
stability: 63.73297286355271
tags: hadoop ?
title: 参考下面的MR系统的场景: --hdfs块的大小为128MB --输入类型为FileInputFormat --有三个文件的大小分别是:64KB 130MB 260MB Hadoop框架会把这些文件拆分为多少块？
type: text/vnd.tiddlywiki

4块：64K，130M，128M，132M