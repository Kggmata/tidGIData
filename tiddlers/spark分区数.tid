created: 20220525033211129
difficulty: 1
due: 20221003150020836
grade: 2
history: [{"due":"20220529150530000","interval":0,"difficulty":2.9854211216321387,"stability":0.21875,"retrievability":1,"grade":-1,"lapses":0,"reps":1,"review":"20220528150529999"},{"due":"20220624135001115","interval":5,"difficulty":1.2753945981095078,"stability":22.488023971305275,"retrievability":0.08997347647736906,"grade":2,"lapses":0,"reps":2,"review":"20220602135001115"}]
interval: 23
lapses: 0
modified: 20220625150020836
reps: 3
retrievability: 0.897843755714891
review: 20220625150020836
stability: 100.35422359618087
tags: spark ?
title: spark分区数
type: text/vnd.tiddlywiki

分区个数由什么决定？

- 读取数据：由读取规则决定

  - 读文件：如果指定了就按照指定的划分最小分区，实际分区按块的个数决定，没有指定最大为2

    `python
    minPartitions = minPartitions or min(self.defaultParallelism, 2)
    ```

  - 读列表：如果指定了就生成指定的分区个数，如果没有指定就为默认的并行度

    ```python
    numSlices = int(numSlices) if numSlices is not None else self.defaultParallelism
    ```

  - 默认并行度：spark.default.parallelism

    ```
    For distributed shuffle operations like reduceByKey and join, the largest number of partitions in a parent RDD. For operations like parallelize with no parent RDDs, it depends on the cluster manager:
    Local mode: number of cores on the local machine
    Mesos fine grained mode: 8
    Others: total number of cores on all executor nodes or 2, whichever is large
`

- 处理数据

  - 不经过shuffle：默认为父RDD的分区个数

  - 经过shuffle但未指定分区个数：默认为父RDD的分区个数

  - 经过shuffle但指定了分区个数：就是指定的分区个数

  - 如果调用repartition或者coalesce：就是调整后的分区个数