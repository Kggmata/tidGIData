created: 20220521054044427
difficulty: 2.500940643340839
due: 20221014094848069
grade: 2
history: [{"due":"20220524024031572","interval":0,"difficulty":5,"stability":2,"retrievability":1,"grade":-1,"lapses":0,"reps":1,"review":"20220522024031572"},{"due":"20220527094132331","interval":4,"difficulty":6.010000000000001,"stability":0.2500261494800798,"retrievability":0.81,"grade":0,"lapses":1,"reps":1,"review":"20220526094132331"},{"due":"20220610145845898","interval":5,"difficulty":4.3316034514288315,"stability":10.231416588301853,"retrievability":0.121603451428831,"grade":2,"lapses":1,"reps":2,"review":"20220531145845898"},{"due":"20220712121914242","interval":12,"difficulty":3.4153606325533037,"stability":30.347651328936035,"retrievability":0.883757181124472,"grade":2,"lapses":1,"reps":3,"review":"20220612121914242"}]
interval: 35
lapses: 1
modified: 20221210010743381
modifier: Yangqing QU
reps: 4
retrievability: 0.8855800107875349
review: 20220717094848069
stability: 89.06463235224017
tags: [[hdfs(hadoop distributed file system)]] ?
title: HDFS的数据压缩算法？及每种算法的应用场景？
type: text/vnd.tiddlywiki

1）gzip压缩

优点：压缩率比较高，而且压缩/解压速度也比较快；hadoop本身支持，在应用中处理gzip格式的文件就和直接处理文本一样；大部分linux系统都自带gzip命令，使用方便。

缺点：不支持split。

应用场景：当每个文件压缩之后在130M以内的（1个块大小内），都可以考虑用gzip压缩格式。例如说一天或者一个小时的日志压缩成一个gzip文件，运行mapreduce程序的时候通过多个gzip文件达到并发。hive程序，streaming程序，和java写的mapreduce程序完全和文本处理一样，压缩之后原来的程序不需要做任何修改。

2）Bzip2压缩

优点：支持split；具有很高的压缩率，比gzip压缩率都高；hadoop本身支持，但不支持native；在linux系统下自带bzip2命令，使用方便。

缺点：压缩/解压速度慢；不支持native。

应用场景：适合对速度要求不高，但需要较高的压缩率的时候，可以作为mapreduce作业的输出格式；或者输出之后的数据比较大，处理之后的数据需要压缩存档减少磁盘空间并且以后数据用得比较少的情况；或者对单个很大的文本文件想压缩减少存储空间，同时又需要支持split，而且兼容之前的应用程序（即应用程序不需要修改）的情况。

3）Lzo压缩

优点：压缩/解压速度也比较快，合理的压缩率；支持split，是hadoop中最流行的压缩格式；可以在linux系统下安装lzop命令，使用方便。

缺点：压缩率比gzip要低一些；hadoop本身不支持，需要安装；在应用中对lzo格式的文件需要做一些特殊处理（为了支持split需要建索引，还需要指定inputformat为lzo格式）。

应用场景：一个很大的文本文件，压缩之后还大于200M以上的可以考虑，而且单个文件越大，lzo优点越越明显。

4）Snappy压缩

优点：高速压缩速度和合理的压缩率。

缺点：不支持split；压缩率比gzip要低；hadoop本身不支持，需要安装；

应用场景：当Mapreduce作业的Map输出的数据比较大的时候，作为Map到Reduce的中间数据的压缩格式；或者作为一个Mapreduce作业的输出和另外一个Mapreduce作业的输入。
