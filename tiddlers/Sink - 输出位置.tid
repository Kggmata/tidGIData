created: 20220612090106990
difficulty: 1
due: 20220924074331518
grade: 2
history: [{"due":"20220616064957532","interval":0,"difficulty":0.956170980055279,"stability":0.10092773437500001,"retrievability":1,"grade":-1,"lapses":0,"reps":1,"review":"20220615064957532"},{"due":"20220701002802596","interval":2,"difficulty":1,"stability":13.526011926209534,"retrievability":0.12395450234099842,"grade":1,"lapses":0,"reps":2,"review":"20220617002802596"}]
interval: 18
lapses: 0
modified: 20230102012724456
modifier: Yangqing QU
reps: 3
retrievability: 0.8691752299029826
review: 20220705074331518
stability: 80.90041734593629
tags: stream实时/sparkStructedStreaming结构化流 ?
title: Sink - 输出位置
type: text/vnd.tiddlywiki

''小总结''

```
- File输出，写出文件
- foreach输出，一条条给你数据，自行处理
- foreachBatch输出，一次给一个批次的DataFrame，自行处理
- memory输出，将无界DF注册为`内存表`，可供SQL二次查询
- kafka sink
- console sink
```

在结构化流中，官方支持5种数据输出的位置

| 名称         | 输出位置      | 备注                             |
| ------------ | ------------- | -------------------------------- |
| File         | 文件          | 支持parquet、csv、json等常见格式 |
| kafka        | kafka消息队列 |                                  |
| foreach      | 自行决定      | 一行行处理                       |
| foreachBatch | 自行决定      | 一批批处理                       |
| memory       | 内存          | 可以供二次查询                   |

1 File输出

以常见的Spark parquet格式为例

```python
# coding:utf8
"""
演示输出到Parquet文件中
"""
from pyspark.sql import SparkSession
from pyspark.sql import DataFrame
import pyspark.sql.functions as F

# 构建入口对象，SparkSession对象
spark = SparkSession.builder.\
    master("local[*]").\
    appName("test spark").\
    config("spark.default.parallelism", 1).\
    config("spark.sql.shuffle.partitions", 1).\
    getOrCreate()

# 使用rate source 生成模拟数据，写入parquet
rate_df = spark.readStream.\
    format("rate").\
    option("rowsPerSecond", 20).\
    option("numPartitions", 2).\
    load()

rate_df.writeStream.\
    outputMode("append").\
    format("parquet").\
    option("path", "../data/output/parquet-output").\
    option("checkpointLocation", "../data/ck2").\
    start().awaitTermination()

```

注意，代码中：

- writeStream和outputMode 2个API和批处理不同
- 其余一致



> 注意：写文件，必须设置CheckPoint的目录



2 foreach输出

```python
# coding:utf8
"""
演示输出位置：foreach算子
"""
from pyspark.sql import SparkSession
from pyspark.sql import DataFrame
import pyspark.sql.functions as F
from pyspark.sql import Row

import os
# PYSPARK_PYTHON告诉Spark，Worker（Executor）应该用哪个Python
os.environ['PYSPARK_PYTHON'] = "/export/server/anaconda3/envs/pyspark/bin/python"
# 告诉Driver应该用哪个Python
os.environ['PYSPARK_DRIVER_PYTHON'] = "/export/server/anaconda3/envs/pyspark/bin/python"
os.environ['JAVA_HOME'] = "/export/server/jdk"


# 构建入口对象，SparkSession对象
spark = SparkSession.builder.\
    master("local[*]").\
    appName("test spark").\
    config("spark.default.parallelism", 1).\
    config("spark.sql.shuffle.partitions", 1).\
    getOrCreate()


line_df: DataFrame = spark.readStream.\
    format("socket").\
    option("host", "node1").\
    option("port", 9999).\
    load()

# 对这个对象进行split切分，进行行转列变成一列的表格（SparkSQL中wordcount的计算是一致的）
word_df = line_df.select(
    F.explode(F.split(line_df.value, " ")).alias("word")
)

# group by 计算单词次数
result_df = word_df.groupBy("word").count()


# TODO: 方式1
def process_row(row: Row):
    # Spark会将一套数据的Row对象传给你，怎么处理自己决定
    print(f"我处理row了，处理方式是输出控制台：{row}")


# TODO: 方式2
class MyForeachProcess:
    def open(self, partition_id, epoch_id):
        """
        Spark会自行调用你这个类的open方法，执行在处理数据前的【前置工作】
        这个模式适合写入数据库
        open开启连接
        close关闭连接
        process插入数据
        :param partition_id: Spark会传给你当前处理的数据所在的分区号
        :param epoch_id: Spark会传给你当前处理数据所在的批次号
        :return: 一定要返回True
        """
        print(f"我open了，当前数据的分区是：{partition_id}，当前的批次是：{epoch_id}")
        return True

    def process(self, row: Row):
        # Spark会将一套数据的Row对象传给你，怎么处理自己决定
        f= open("/tmp/aaaaa.txt", "a", encoding="UTF8")
        f.write(f"{row}")
        f.write("\n")
        f.close()

    def close(self, error):
        """
        如果报错了，会自动调用close方法，处理完成了也会调用
        一般我们在这里面可以关闭链接、回滚事务
        :param error:
        :return:
        """
        print(f"我close了。")

# 通过foreach去输出
result_df.writeStream.\
    outputMode("complete").\
    foreach(MyForeachProcess()).\
    start().awaitTermination()

```

>- 方式1：适合和数据库无关的操作
>- 方式2：由于带open和close可以在处理数据前做前置工作（打开数据库链接），以及做收尾工作（关闭链接、回滚等）





3 foreachBatch输出

将一个批次的数据（有界DataFrame）提供给你。

你自己准备一个处理方法，接收Spark传给你的有界DF，自行处理即可（爱怎么处理怎么处理）



```python
# coding:utf8
"""
演示输出位置：foreach_batch算子
"""
from pyspark.sql import SparkSession
from pyspark.sql import DataFrame
import pyspark.sql.functions as F
from pyspark.sql import Row

import os
# PYSPARK_PYTHON告诉Spark，Worker（Executor）应该用哪个Python
os.environ['PYSPARK_PYTHON'] = "/export/server/anaconda3/envs/pyspark/bin/python"
# 告诉Driver应该用哪个Python
os.environ['PYSPARK_DRIVER_PYTHON'] = "/export/server/anaconda3/envs/pyspark/bin/python"
os.environ['JAVA_HOME'] = "/export/server/jdk"


# 构建入口对象，SparkSession对象
spark = SparkSession.builder.\
    master("local[*]").\
    appName("test spark").\
    config("spark.default.parallelism", 1).\
    config("spark.sql.shuffle.partitions", 1).\
    getOrCreate()


line_df: DataFrame = spark.readStream.\
    format("socket").\
    option("host", "node1").\
    option("port", 9999).\
    load()

# 对这个对象进行split切分，进行行转列变成一列的表格（SparkSQL中wordcount的计算是一致的）
word_df = line_df.select(
    F.explode(F.split(line_df.value, " ")).alias("word")
)

# group by 计算单词次数
result_df = word_df.groupBy("word").count()


def my_foreach_batch_func(df: DataFrame, batch_id):
    """
    Spark会将当前批次的DataFrame传入，自行处理
    以及当前批次的批次号
    :param df: 当前批次的DataFrame数据
    :param batch_id: 当前批次的批次号
    :return: 无
    """
    # 在这个方法内部，是批处理了
    print(f"当前批次是：{batch_id}，内容：")
    df.show()


# 通过foreach去输出
result_df.writeStream.\
    outputMode("complete").\
    foreachBatch(my_foreach_batch_func).\
    start().awaitTermination()

```



5 memory输出

将内容输出到 `内存中`，作为一个临时的有界DF存在，可供二次执行SQL查询

> 也就是，每一个批次的无界DF的结果，临时放入内存，供你用自定义的SQL进行二次查询



```python
# coding:utf8
"""
演示输出位置：输出到内存中，供二次查询
"""
import time

from pyspark.sql import SparkSession
from pyspark.sql import DataFrame
import pyspark.sql.functions as F
from pyspark.sql import Row

import os
# PYSPARK_PYTHON告诉Spark，Worker（Executor）应该用哪个Python
os.environ['PYSPARK_PYTHON'] = "/export/server/anaconda3/envs/pyspark/bin/python"
# 告诉Driver应该用哪个Python
os.environ['PYSPARK_DRIVER_PYTHON'] = "/export/server/anaconda3/envs/pyspark/bin/python"
os.environ['JAVA_HOME'] = "/export/server/jdk"


# 构建入口对象，SparkSession对象
spark = SparkSession.builder.\
    master("local[*]").\
    appName("test spark").\
    config("spark.default.parallelism", 1).\
    config("spark.sql.shuffle.partitions", 1).\
    getOrCreate()


line_df: DataFrame = spark.readStream.\
    format("socket").\
    option("host", "node1").\
    option("port", 9999).\
    load()

# 对这个对象进行split切分，进行行转列变成一列的表格（SparkSQL中wordcount的计算是一致的）
word_df = line_df.select(
    F.explode(F.split(line_df.value, " ")).alias("word")
)

# group by 计算单词次数
result_df = word_df.groupBy("word").count()


# 通过输出到内存中，需要用二次查询，
# 1. format选择memory
# 2. 需要调用queryName，给内存的临时表起名字
query = result_df.writeStream.\
    outputMode("complete").\
    format("memory").\
    queryName("my_memory_table").\
    start()

while (1 == 1):
    # 如果不是无循环，只查询一次就结束
    # 由于通过queryName设置了临时的表名，直接spark.sql直接查即可
    spark.sql("SELECT * FROM my_memory_table ORDER BY COUNT DESC").\
        show()
    time.sleep(1)


# awaitTermination不在start后面调用了
# 而是在对内存表进行二次查询的后面
# 或者不写也行，无限循环，下面这一条语句 也执行不到。
query.awaitTermination()
```
[img[20220612164659.png]]


