created: 20220623060957575
difficulty: 1
due: 20221025054405229
grade: 2
history: [{"due":"20220625233409481","interval":0,"difficulty":0.4015068894836844,"stability":0.1000072479248047,"retrievability":1,"grade":-1,"lapses":0,"reps":1,"review":"20220624233409481"},{"due":"20220715004940504","interval":4,"difficulty":1,"stability":16.061313745994745,"retrievability":0.014785398243253882,"grade":2,"lapses":0,"reps":2,"review":"20220629004940504"}]
interval: 22
lapses: 0
modified: 20220721054405229
reps: 3
retrievability: 0.865612726934808
review: 20220721054405229
stability: 95.61179776281308
tags: 用户画像 ?
title: Hadoop的默认块大小是多少？为什么要设置这么大？
type: text/vnd.tiddlywiki

默认块大小：

```
Hadoop2.7.2版本及之前默认64MB，Hadoop2.7.3版本及之后默认128M
```
''最佳传输损耗理论：在一次传输中，寻址时间占用总传输时间的1%时，本次传输的损耗最小，为最佳性价比传输！''

块大小：

```
HDFS中平均寻址时间大概为10ms；

经过前人的大量测试发现，寻址时间为传输时间的1%时，为最佳状态；

所以最佳传输时间为10ms/0.01=1000ms=1s

目前磁盘的传输速率普遍为100MB/s；

计算出最佳block大小：100MB/s x 1s = 100MB

所以我们设定block大小为128MB。

ps：实际在工业生产中，磁盘传输速率为200MB/s时，一般设定block大小为256MB，磁盘传输速率为400MB/s时，一般设定block大小为512MB
```

默认的块大小128MB。块的大小：10ms100100M/s=100M，如图
[img[在这里插入图片描述2022-06-23 14_10_21.png]]

块大小需要合适调节

```
不能太大：
当前有文件a, 还有1G内存
假设当前只需要读取a文件0-128M部分的内容
128M一块 1G可以存8块 ， 取第一块
1G一块 1G存可以1块 ， 取第一块
①在一些分块读取的场景，不够灵活，会带来额外的网络消耗
②在上传文件时，一旦发生故障，会造成资源的浪费

不能太小：
比如文件a,128M
1M一块： 128个块，生成128个块的映射信息
128M一块， 1个块，一个块的映射信息
①块太小，同样大小的文件，会占用过多的NN的元数据空间
②块太小，在进行读写操作时，会消耗额外的寻址时间
```
————————————————
版权声明：本文为CSDN博主「二十六画生的博客」的原创文章，遵循CC 4.0 BY-SA版权协议，转载请附上原文出处链接及本声明。
原文链接：https://blog.csdn.net/u010002184/article/details/113247735