created: 20220618090431586
difficulty: 1
due: 20220825153441222
grade: 2
history: [{"due":"20220621004020344","interval":0,"difficulty":0.5718632935728973,"stability":0.10005798339843751,"retrievability":1,"grade":-1,"lapses":0,"reps":1,"review":"20220620004020344"},{"due":"20220630144823854","interval":1,"difficulty":1,"stability":8.830517274901752,"retrievability":0.348891394928074,"grade":2,"lapses":0,"reps":2,"review":"20220621144823854"}]
interval: 11
lapses: 0
modified: 20220702153441222
reps: 3
retrievability: 0.8770024421488429
review: 20220702153441222
stability: 53.686353537936405
tags: 用户画像 ?
title: 保险的复杂的计算场景，可以选用比如hive、spark、flink这些复杂的框架。
type: text/vnd.tiddlywiki

* 不用hive，是因为MR太慢了。一个大的job会拆成多个连续的MapTask和ReduceTask，上一个MapTask的中间结果会写到磁盘，下一个ReduceTask读取磁盘的数据，频繁的磁盘IO造成慢。
* Hive的Task是以进程为单位的，进程的启动和销毁消耗更多的时间和资源。
* Flink更多适合做实时流处理。保险项目是批处理场景
* 所以采用Spark来做。
* 选择SparkOnhive
* 问题1：SparkOnHive如何搭建？
* 技术发展：Hive---->Shark(Hive On Spark)---> Spark(包括spark On HIve)
* Shark(Hive On Spark)就是将hive的底层执行引擎调整spark，但是大量hive代码没有重构，后续shark淘汰
* spark On Hive（可以在Spark.sql(hivesql)，底层完全使用Spark的执行引擎）