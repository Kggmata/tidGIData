created: 20220724085736537
creator: Yangqing QU
difficulty: 1
due: 20221202121659044
grade: 0
history: [{"due":"20220827142944227","interval":0,"difficulty":0.025386988635295055,"stability":0.10000000088475645,"retrievability":1,"grade":-1,"lapses":0,"reps":1,"review":"20220826142944227"}]
interval: 97
lapses: 1
modified: 20230102012832561
modifier: Yangqing QU
reps: 1
retrievability: 4.1232014016533527e-45
review: 20221201121659044
stability: 1.4816364413634358
tags: stream实时/flink ?
title: FLinkCDC1.x痛点
type: text/vnd.tiddlywiki

```
Flink CDC 1.0 底层封装了 Debezium， Debezium 同步一张表分为两个阶段: 
1)	全量阶段：查询当前表中所有记录；
2)	增量阶段：从 binlog 消费变更数据。
	大部分用户使用的场景都是全量 + 增量同步，加锁是发生在全量阶段，目的是为了确定全量阶段的初始位点，保证增量 + 全量实现一条不多，一条不少，从而保证数据一致性。从下图2.9我们可以分析全局锁和表锁的一些加锁流程，左边红色线条是锁的生命周期，右边是 MySQL 开启可重复读事务的生命周期。

```

[img[BillfishnKwvxD2022-07-24 16_57_54.PNG]]

```
以全局锁为例，首先是获取一个锁，然后再去开启可重复读的事务。这里锁住操作是读取 binlog 的起始位置和当前表的 schema。这样做的目的是保证 binlog 的起始位置和读取到的当前 schema 是可以对应上的，因为表的 schema 是会改变的，比如如删除列或者增加列。在读取这两个信息后，SnapshotReader 会在可重复读事务里读取全量数据，在全量数据读取完成后，会启动 BinlogReader 从读取的 binlog 起始位置开始增量读取，从而保证全量数据 + 增量数据的无缝衔接。
	表锁是全局锁的退化版，因为全局锁的权限会比较高，因此在某些场景，用户只有表锁。表锁锁的时间会更长，因为表锁有个特征：锁提前释放了可重复读的事务默认会提交，所以锁需要等到全量数据读完后才能释放。
经过上面分析，接下来看看这些锁到底会造成怎样严重的后果：
FLUSH TABLES WITH READ LOCK 
1)	该命令等待所有正在进行的update完成，同时阻止所有新来的update。
2)	该命令执行成功前必须等待所有正在运行的select 完成，所有等待执行的update 会等待的更久。更坏的情况是，在等待正在运行select完成时，DB实际上处于不可用状态，即使是新加入的SELECT也会被阻止。这是MySQL Query Cache机制。
3)	该命令阻止其他事务commit。
结论: 加锁时间是不确定的，极端情况会hang 住数据库
Flink CDC 1.x 可以不加锁，能够满足大部分场景，但牺牲了一定的数据准确性。Flink CDC 1.x 默认加全局锁，虽然能保证数据一致性，但存在上述 hang 住数据的风险。
随着 Flink CDC 项目的发展，得到了很多用户在社区的反馈，主要归纳为三个:
1)	全量 + 增量读取的过程需要保证所有数据的一致性，因此需要通过加锁保证，但是加锁在数据库层面上是一个十分高危的操作。底层 Debezium 在保证数据一致性时，需要对读取的库或表加锁，全局锁可能导致数据库锁住，表级锁会锁住表的读，DBA 一般不给锁权限。
2)	不支持水平扩展，因为 Flink CDC 1.0 底层是基于 Debezium，架构是单节点，所以Flink CDC 1.0只支持单并发。在全量阶段读取阶段，如果表非常大 (亿级别)，读取时间在小时甚至天级别，用户不能通过增加资源去提升作业速度。
3)	全量读取阶段不支持 checkpoint：CDC 读取分为两个阶段，全量读取和增量读取，目前全量读取阶段是不支持 checkpoint 的，因此会存在一个问题：当我们同步全量数据时，假设需要 5 个小时，当我们同步了 4 小时的时候作业失败，这时候就需要重新开始，再读取 5 个小时。

```
